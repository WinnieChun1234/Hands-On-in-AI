{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Checkpoint1c_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24b49243c88a45bab17bc22797268880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ec14379e63c484792373719c43eb254",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ef56554d120840058a16cca8a6a02a96",
              "IPY_MODEL_34b9cfa193184580b7359daf11a52b69"
            ]
          }
        },
        "7ec14379e63c484792373719c43eb254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef56554d120840058a16cca8a6a02a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0de144c8382d4610ac7a5b4810063432",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f204db636383456ca636f7ba9ef5b0e0"
          }
        },
        "34b9cfa193184580b7359daf11a52b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_af428b610ab34c07960da1843103cc16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 29940705.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e809e76c8bef4cada950b786e35c10ae"
          }
        },
        "0de144c8382d4610ac7a5b4810063432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f204db636383456ca636f7ba9ef5b0e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af428b610ab34c07960da1843103cc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e809e76c8bef4cada950b786e35c10ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv9TM291deFd"
      },
      "source": [
        "### COMP3359 Artificial Intelligence Applications\n",
        "Department of Computer Science, HKU\n",
        "<br><br>\n",
        "\n",
        "# <u>Checkpoint 1c - PyTorch</u>  \n",
        "\n",
        "\n",
        "## Estimated Time to Finish:   \n",
        "1~2 hours   \n",
        "\n",
        "## Main Learning Objectives:   \n",
        "-\tPractise usage of common ML framework to construct simple application.\n",
        "\n",
        "## Overview   \n",
        "1.\t[Introduction](#s1)  \n",
        "2.\t[Before You Start](#s2)\n",
        "3.\t[Preparation](#s3)\n",
        "4.\t[Task - Reuse Model and Make Predictions](#s4)\n",
        "5.\t[Submission](#s5)\n",
        "\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j9ETFDQbDoI"
      },
      "source": [
        "<a id=’s1’></a>\n",
        "# 1 Introduction\n",
        "\n",
        "It may be a good idea to kick-start our study of building AI applications by learning basic usage of some ML framework. This checkpoint extends our other material \"ML Framework Learning Roadmap - PyTorch\". The main task in this checkpoint is to reuse the model trained in the example in \"ML Framework Learning Roadmap - PyTorch\", and give predictions to data we provide.\n",
        "\n",
        "-----   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XP0D1ULzZhm"
      },
      "source": [
        "<a id=’s2’></a>\n",
        "# 2 Before You Start\n",
        "\n",
        "## Referenced Tutorial\n",
        "\n",
        "This checkpoint is mainly built by referencing the following tutorial. It is <b>strongly recommended</b> to study it once first to understand the context of this tutoiral.\n",
        "\n",
        "Referenced tutorial:\n",
        "- [PyTorch - TRAINING A CLASSIFIER](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
        "\n",
        "## Auxilliary Tools\n",
        "\n",
        "In this checkpoint, you may need to use python packages to help you tackle the problems. <b>If you have no experience</b> using the following packages, it is <b>recommended</b> to check the following short tutorials and complete the simple exercises inside.\n",
        "\n",
        "- NumPy\n",
        "    - [NumPy UltraQuick Tutorial](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb)\n",
        "- Pandas\n",
        "    - [Pandas DataFrame UltraQuick Tutorial](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "\n",
        "Since in the context of AI, we often handles a large number of numerical values that are arraged as <b>(multi-dimensional) arrays</b> (e.g. vectors, matrices, tensors), you may pay attentation to the manipulations of such (multi-dimensional) arrays in these tutorials.\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WixLN2pDdhak"
      },
      "source": [
        "<a id=’s3’></a>\n",
        "# 3 Preparation\n",
        "\"ML Framework Learning Roadmap - PyTorch\" suggests a tutorial about training an image classifier model. To prepare ourselves for this checkpoint, we trained the same model here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511,
          "referenced_widgets": [
            "24b49243c88a45bab17bc22797268880",
            "7ec14379e63c484792373719c43eb254",
            "ef56554d120840058a16cca8a6a02a96",
            "34b9cfa193184580b7359daf11a52b69",
            "0de144c8382d4610ac7a5b4810063432",
            "f204db636383456ca636f7ba9ef5b0e0",
            "af428b610ab34c07960da1843103cc16",
            "e809e76c8bef4cada950b786e35c10ae"
          ]
        },
        "id": "DnyZ0Y7_pAvl",
        "outputId": "80400cdf-b285-427b-e69a-ede1c4bc59d4"
      },
      "source": [
        "# Ref.: PyTorch - TRAINING A CLASSIFIER\n",
        "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
        " \n",
        "# Loading and normalizing CIFAR10\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        " \n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        " \n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        " \n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        " \n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        " \n",
        "print(\"===== Data =====\")\n",
        "print(\"# training samples: \", len(trainset))\n",
        "print(\"# training batches: \", len(trainloader))\n",
        "print(\"# test samples: \", len(testset))\n",
        "print(\"# test batches: \", len(testloader))\n",
        "print(\"(class label, class name): \")\n",
        "print(list(zip(range(len(classes)), classes)))\n",
        " \n",
        " \n",
        "# Define a Convolutional Neural Network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        " \n",
        "net = Net()\n",
        " \n",
        "# Define a Loss function and optimizer\n",
        "import torch.optim as optim\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        " \n",
        "# Train the network\n",
        "print(\"===== Model Training =====\")\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        " \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        " \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        " \n",
        "print('Finished Training')\n",
        " \n",
        "# Test the network on the test data\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        " \n",
        "\"\"\"\n",
        "# Evaluate on individual images\n",
        " \n",
        "# print images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        " \n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        " \n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))\n",
        " \n",
        "\"\"\"\n",
        "# Evaluate on entire test dataset\n",
        "print(\"===== Model Evaluation =====\")\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        " \n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24b49243c88a45bab17bc22797268880",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "===== Data =====\n",
            "# training samples:  50000\n",
            "# training batches:  12500\n",
            "# test samples:  10000\n",
            "# test batches:  2500\n",
            "(class label, class name): \n",
            "[(0, 'plane'), (1, 'car'), (2, 'bird'), (3, 'cat'), (4, 'deer'), (5, 'dog'), (6, 'frog'), (7, 'horse'), (8, 'ship'), (9, 'truck')]\n",
            "===== Model Training =====\n",
            "[1,  2000] loss: 2.193\n",
            "[1,  4000] loss: 1.891\n",
            "[1,  6000] loss: 1.712\n",
            "[1,  8000] loss: 1.602\n",
            "[1, 10000] loss: 1.514\n",
            "[1, 12000] loss: 1.479\n",
            "[2,  2000] loss: 1.406\n",
            "[2,  4000] loss: 1.389\n",
            "[2,  6000] loss: 1.359\n",
            "[2,  8000] loss: 1.317\n",
            "[2, 10000] loss: 1.295\n",
            "[2, 12000] loss: 1.311\n",
            "Finished Training\n",
            "===== Model Evaluation =====\n",
            "Accuracy of the network on the 10000 test images: 53 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wvyiPcYKdkX"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_j4Ld4MKnld"
      },
      "source": [
        "<a id=’s4’></a>\n",
        "# 4 Reuse Model and Make Predictions\n",
        "In the previous section, we have trained a model for image recognition. Next, we will try to reuse the trained model and make predictions on the images we provide for this checkpoint.\n",
        "\n",
        "We have prepared a data directory with few test images to be classified. The following sample codes assume the data directory is located next to this notebook, e.g.:\n",
        "```\n",
        "..\n",
        "|-- Checkpoint1_PyTorch.ipynb\n",
        "|-- images/\n",
        "  |-- img0.jpg\n",
        "  # more images…\n",
        "```\n",
        "If you put your data directory in somewhere else, you will need to modify the path to data directory accordingly below.\n",
        "\n",
        "Our task is to <u><b> make predictions on the provided images using the trained model</b></u>. More specifically, we want to print out predictions as in:\n",
        "```\n",
        "# … all preceding steps.\n",
        "# Print out predicted class names \n",
        "pred_class_names = [ classes[pl] for pl in pred_labels ]\n",
        "print(\"Predictions: \", pred_class_names)\n",
        "```\n",
        "and your task here is to <u>complete the steps before printing out predictions</u>, which are briefly:\n",
        "1.\tLoad image data.\n",
        "2.\tPreprocess data.\n",
        "3.\tMake predictions using trained model.\n",
        "\n",
        "There are more than one way to carry out these steps and <u>you are welcomed to prepare predictions in your own fashion</u>. In case you are feeling uncertain about where to start, in the next code cell an example procedure is provided for you, and <u>you could complete the task by filling in the missing parts according to instructions given</u>. \n",
        "\n",
        "In addition, the following materials may help you to complete this task:\n",
        "-\t[Dataset Class - WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS]( https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)\n",
        "-\t[How to read just one pic?](https://discuss.pytorch.org/t/how-to-read-just-one-pic/17434)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjtUsj9AKdJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548fe794-0085-4a97-9790-7afeab939e17"
      },
      "source": [
        "# Mount Google Drive for loading data if running in Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI8zHrIlLFuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d04bf74-cd78-41f1-8204-044dd1fd454f"
      },
      "source": [
        "\"\"\" Reuse Trained Model and Make Prediction \"\"\"\n",
        "########################################################\n",
        "# This is only a suggested method to make predictions  #\n",
        "# on the provided data.                                #\n",
        "#                                                      #\n",
        "# You may modify or replace the following codes,       #\n",
        "# as long as you can provide predictions from          #\n",
        "# the trained model.                                   #\n",
        "########################################################\n",
        "\n",
        "# Specify data directory\n",
        "#data_dir = \"./images/\"\n",
        "data_dir = '/content/drive/MyDrive/images'\n",
        "\n",
        "import torch\n",
        "import skimage\n",
        "from skimage import io\n",
        "\n",
        "# Get image paths in data_dir\n",
        "import os\n",
        "data_paths = os.listdir(data_dir)\n",
        "data_paths = [ os.path.join(data_dir, p) for p in data_paths ]\n",
        "data_paths.sort()\n",
        "\n",
        "##### Load Images #####\n",
        "# Load all images into one big tensor\n",
        "image_batch = []\n",
        "for path in data_paths :\n",
        "  print(path)\n",
        "  # Load image file. You may try:\n",
        "  #     skimage.io.imread: https://scikit-image.org/docs/dev/api/skimage.io.html?highlight=imread#skimage.io.imsave\n",
        "  image = skimage.io.imread(path, as_gray=False)\n",
        "  \n",
        "\n",
        "  # Convert to PyTorch tensor. \n",
        "  # You may need to specify tensor as \"float\" type, which is required by our model.\n",
        "  image_tensor = torch.Tensor(image)\n",
        "  \n",
        "  # As PyTorch Conv2d Layer operates on input with shape (N,C,H,W),\n",
        "  # where N: batch size, C: #channels (e.g. RGB), H: height, W: width,\n",
        "  # you may need to convert shape of image we just loaded from (H,W,C) to (C,H,W).\n",
        "  # (see https://pytorch.org/docs/stable/nn.html#conv2d)\n",
        "  # To change order of tensor dimensions, you may try: \n",
        "  #     torch.Tensor.permute: https://pytorch.org/docs/stable/nn.html#conv2d\n",
        "  image_tensor = image_tensor.permute([2,0,1])\n",
        "  \n",
        "  # Store loaded image in our list\n",
        "  image_batch.append(image_tensor) \n",
        "\n",
        "# Pack list of image tensors into one big tensor.\n",
        "# (p.s. image_batch should has shape (B,C,H,W))\n",
        "image_batch = torch.stack(image_batch)\n",
        "\n",
        "##### Make Predictions using Trained Model #####\n",
        "# Make predictions on images in 'image_batch' \n",
        "\n",
        "# Disable gradient update engine during evaluation phase \n",
        "with torch.no_grad() :\n",
        "  # Make predictions using trained model\n",
        "  #pred_labels = [net(images) for images in image_batch]\n",
        "  _,pred_labels = torch.max(net(image_batch), 1)\n",
        "\n",
        "\n",
        "############################################################\n",
        "# Printing out predictions here is your goal of this task. #\n",
        "############################################################\n",
        "# Print out predicted class names\n",
        "pred_class_names = [ classes[pl] for pl in pred_labels ]\n",
        "print(\"Predictions: \", pred_class_names)\n",
        "\n",
        "# Predictions:  ['ship', 'car', '?', 'truck', 'ship', '?', '?']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/images/img0.jpg\n",
            "/content/drive/MyDrive/images/img1.jpg\n",
            "/content/drive/MyDrive/images/img2.jpg\n",
            "/content/drive/MyDrive/images/img3.jpg\n",
            "/content/drive/MyDrive/images/img4.jpg\n",
            "/content/drive/MyDrive/images/img5.jpg\n",
            "/content/drive/MyDrive/images/img6.jpg\n",
            "Predictions:  ['ship', 'car', 'plane', 'truck', 'ship', 'plane', 'horse']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QspevTsUb1SK"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Eq0USSPb5Ge"
      },
      "source": [
        "<a id=’s5’></a>\n",
        "# 5 Submission\n",
        "\n",
        "\n",
        "To complete and submit your work, please submit the completed version of this notebook to Moodle. Please make sure that it can be executed without errors, and predictions from trained model are provided in clear, comprehensible fashion.\n",
        "\n",
        "\n",
        "-----\n"
      ]
    }
  ]
}